{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6179b320-119f-477d-9ccf-48798ad205bd",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive guide to using the trained AtomGPT model for making predictions on new atomic structure descriptions. The key steps include:\n",
    "\n",
    "Setup and Imports: Import necessary libraries and set the computation device.\n",
    "\n",
    "Load Configuration: Load the training configuration to ensure consistency.\n",
    "\n",
    "Initialize Tokenizer and Model: Set up the tokenizer and model architecture, then load the trained weights.\n",
    "\n",
    "Define the Prediction Dataset Class: Create a dataset class to handle input descriptions.\n",
    "\n",
    "Prepare Input Data for Prediction: Provide the descriptions for which you want to predict properties.\n",
    "\n",
    "Create DataLoader for Predictions: Batch the input data for efficient processing.\n",
    "\n",
    "Make Predictions: Pass the input through the model to obtain predictions.\n",
    "\n",
    "Process and Save Predictions: Organize the predictions and save them for further analysis.\n",
    "\n",
    "(Optional) Visualize Predictions: Visualize the results to gain insights.\n",
    "\n",
    "Note: Ensure that all file paths (e.g., config_path, input_descriptions.csv) are correctly set based on your project structure. Additionally, adjust the input_descriptions list to include the actual descriptions you wish to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5052696-a975-4388-b134-f0160e05fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, T5Tokenizer, T5ForConditionalGeneration\n",
    "from jarvis.db.jsonutils import loadjson, dumpjson\n",
    "from jarvis.core.atoms import Atoms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4b2f6-2576-4730-ab80-ed2dcf356298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the configuration file\n",
    "config_path = \"out_temp/config.json\"  # Update this path as needed\n",
    "\n",
    "# Load the configuration\n",
    "config = loadjson(config_path)\n",
    "print(\"Loaded Configuration:\")\n",
    "print(json.dumps(config, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6832102-9714-467a-a31a-7d03083c09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract necessary configurations\n",
    "model_name = config['model_name']\n",
    "pretrained_path = os.path.join(config['output_dir'], \"best_model.pt\")\n",
    "desc_type = config['desc_type']\n",
    "\n",
    "# Initialize the tokenizer based on the model\n",
    "if \"t5\" in model_name:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add special tokens if they were added during training\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    tokenizer.add_special_tokens({\"unk_token\": \"#\"})\n",
    "    tokenizer.add_special_tokens({\"unk_token\": \"&\"})\n",
    "    tokenizer.add_special_tokens({\"unk_token\": \"@\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Modify the model's language modeling head for regression (same as training)\n",
    "latent_dim = config['latent_dim']\n",
    "model.lm_head = torch.nn.Sequential(\n",
    "    torch.nn.Linear(model.config.hidden_size, latent_dim),\n",
    "    torch.nn.Linear(latent_dim, 1),\n",
    ")\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load(pretrained_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5680b-22cd-42ba-99d9-787b30ad0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        \"\"\"Initialize the dataset with texts and tokenizer.\"\"\"\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieve a single sample by index.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ce3fa-639a-460b-88ac-6ee310c1658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input descriptions\n",
    "input_descriptions = [\n",
    "    \"Description of material 1 with atomic structure details...\",\n",
    "    \"Description of material 2 with atomic structure details...\",\n",
    "    # Add more descriptions as needed\n",
    "]\n",
    "\n",
    "# Alternatively, load descriptions from a JSON or CSV file\n",
    "# For example:\n",
    "# descriptions_df = pd.read_csv(\"input_descriptions.csv\")\n",
    "# input_descriptions = descriptions_df['desc'].tolist()\n",
    "\n",
    "print(f\"Number of input descriptions: {len(input_descriptions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698841ba-4805-43c9-9d2f-9c5fc29ac25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the prediction dataset and dataloader\n",
    "prediction_dataset = PredictionDataset(\n",
    "    texts=input_descriptions,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config['max_length']\n",
    ")\n",
    "\n",
    "batch_size = config['batch_size']  # Use the same batch size as training\n",
    "\n",
    "prediction_dataloader = DataLoader(prediction_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a1b69-66f8-4f41-ba81-66c88476fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(prediction_dataloader, desc=\"Making Predictions\"):\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)  # Remove extra dimension and move to device\n",
    "\n",
    "        # Forward pass through the model\n",
    "        if \"t5\" in model_name:\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                decoder_input_ids=input_ids\n",
    "            )\n",
    "            logits = outputs.logits.squeeze(-1).mean(dim=-1)\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            logits = outputs.logits.squeeze(-1).mean(dim=-1)\n",
    "\n",
    "        # Apply the regression head\n",
    "        regression_output = model.lm_head(logits)\n",
    "\n",
    "        # Move predictions to CPU and convert to numpy\n",
    "        preds = regression_output.cpu().numpy().tolist()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "print(\"Predictions completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54758129-97d5-45e3-a49b-494dff6af08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with input descriptions and their corresponding predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'Description': input_descriptions,\n",
    "    'Predicted_Property': predictions\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output_predictions_path = os.path.join(config['output_dir'], \"predictions.csv\")\n",
    "results_df.to_csv(output_predictions_path, index=False)\n",
    "print(f\"Predictions saved to {output_predictions_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51c87d-10d7-433c-b7f9-ee567353ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example: Plot distribution of predicted properties\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(results_df['Predicted_Property'], bins=30, kde=True)\n",
    "plt.title('Distribution of Predicted Properties')\n",
    "plt.xlabel('Predicted Property')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# If you have actual properties for comparison:\n",
    "# results_df['Actual_Property'] = actual_properties_list\n",
    "# sns.scatterplot(data=results_df, x='Actual_Property', y='Predicted_Property')\n",
    "# plt.title('Actual vs Predicted Properties')\n",
    "# plt.xlabel('Actual Property')\n",
    "# plt.ylabel('Predicted Property')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
